--- rest26/howto/urllib2.rst	2008-12-19 16:58:09.000000000 +0900
+++ rest262/howto/urllib2.rst	2009-04-05 05:01:49.000000000 +0900
@@ -1,36 +1,36 @@
 ************************************************
   HOWTO Fetch Internet Resources Using urllib2
 ************************************************
 
 :Author: `Michael Foord <http://www.voidspace.org.uk/python/index.shtml>`_
 
 .. note::
 
     There is an French translation of an earlier revision of this
     HOWTO, available at `urllib2 - Le Manuel manquant
     <http://www.voidspace.org.uk/python/articles/urllib2_francais.shtml>`_.
 
- 
+
 
 Introduction
 ============
 
 .. sidebar:: Related Articles
 
     You may also find useful the following article on fetching web resources
     with Python :
-    
+
     * `Basic Authentication <http://www.voidspace.org.uk/python/articles/authentication.shtml>`_
-    
+
         A tutorial on *Basic Authentication*, with examples in Python.
 
 **urllib2** is a `Python <http://www.python.org>`_ module for fetching URLs
 (Uniform Resource Locators). It offers a very simple interface, in the form of
 the *urlopen* function. This is capable of fetching URLs using a variety of
 different protocols. It also offers a slightly more complex interface for
 handling common situations - like basic authentication, cookies, proxies and so
 on. These are provided by objects called handlers and openers.
 
 urllib2 supports fetching URLs for many "URL schemes" (identified by the string
 before the ":" in URL - for example "ftp" is the URL scheme of
 "ftp://python.org/") using their associated network protocols (e.g. FTP, HTTP).
@@ -89,25 +89,25 @@
 
 Sometimes you want to send data to a URL (often the URL will refer to a CGI
 (Common Gateway Interface) script [#]_ or other web application). With HTTP,
 this is often done using what's known as a **POST** request. This is often what
 your browser does when you submit a HTML form that you filled in on the web. Not
 all POSTs have to come from forms: you can use a POST to transmit arbitrary data
 to your own application. In the common case of HTML forms, the data needs to be
 encoded in a standard way, and then passed to the Request object as the ``data``
 argument. The encoding is done using a function from the ``urllib`` library
 *not* from ``urllib2``. ::
 
     import urllib
-    import urllib2  
+    import urllib2
 
     url = 'http://www.someserver.com/cgi-bin/register.cgi'
     values = {'name' : 'Michael Foord',
               'location' : 'Northampton',
               'language' : 'Python' }
 
     data = urllib.urlencode(values)
     req = urllib2.Request(url, data)
     response = urllib2.urlopen(req)
     the_page = response.read()
 
 Note that other encodings are sometimes required (e.g. for file upload from HTML
@@ -152,47 +152,47 @@
 Some websites [#]_ dislike being browsed by programs, or send different versions
 to different browsers [#]_ . By default urllib2 identifies itself as
 ``Python-urllib/x.y`` (where ``x`` and ``y`` are the major and minor version
 numbers of the Python release,
 e.g. ``Python-urllib/2.5``), which may confuse the site, or just plain
 not work. The way a browser identifies itself is through the
 ``User-Agent`` header [#]_. When you create a Request object you can
 pass a dictionary of headers in. The following example makes the same
 request as above, but identifies itself as a version of Internet
 Explorer [#]_. ::
 
     import urllib
-    import urllib2  
-    
+    import urllib2
+
     url = 'http://www.someserver.com/cgi-bin/register.cgi'
-    user_agent = 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)' 
+    user_agent = 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'
     values = {'name' : 'Michael Foord',
               'location' : 'Northampton',
               'language' : 'Python' }
     headers = { 'User-Agent' : user_agent }
-    
+
     data = urllib.urlencode(values)
     req = urllib2.Request(url, data, headers)
     response = urllib2.urlopen(req)
     the_page = response.read()
 
 The response also has two useful methods. See the section on `info and geturl`_
 which comes after we have a look at what happens when things go wrong.
 
 
 Handling Exceptions
 ===================
 
 *urlopen* raises :exc:`URLError` when it cannot handle a response (though as usual
-with Python APIs, builtin exceptions such as 
+with Python APIs, builtin exceptions such as
 :exc:`ValueError`, :exc:`TypeError` etc. may also
 be raised).
 
 :exc:`HTTPError` is the subclass of :exc:`URLError` raised in the specific case of
 HTTP URLs.
 
 URLError
 --------
 
 Often, URLError is raised because there is no network connection (no route to
 the specified server), or the specified server doesn't exist.  In this case, the
 exception raised will have a 'reason' attribute, which is a tuple containing an
@@ -300,36 +300,36 @@
               'The server cannot process the request due to a high load'),
         504: ('Gateway Timeout',
               'The gateway server did not receive a timely response'),
         505: ('HTTP Version Not Supported', 'Cannot fulfill request.'),
         }
 
 When an error is raised the server responds by returning an HTTP error code
 *and* an error page. You can use the :exc:`HTTPError` instance as a response on the
 page returned. This means that as well as the code attribute, it also has read,
 geturl, and info, methods. ::
 
     >>> req = urllib2.Request('http://www.python.org/fish.html')
-    >>> try: 
+    >>> try:
     >>>     urllib2.urlopen(req)
     >>> except URLError, e:
     >>>     print e.code
     >>>     print e.read()
-    >>> 
+    >>>
     404
-    <!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" 
+    <!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
         "http://www.w3.org/TR/html4/loose.dtd">
-    <?xml-stylesheet href="./css/ht2html.css" 
+    <?xml-stylesheet href="./css/ht2html.css"
         type="text/css"?>
-    <html><head><title>Error 404: File Not Found</title> 
+    <html><head><title>Error 404: File Not Found</title>
     ...... etc...
 
 Wrapping it Up
 --------------
 
 So if you want to be prepared for :exc:`HTTPError` *or* :exc:`URLError` there are two
 basic approaches. I prefer the second approach.
 
 Number 1
 ~~~~~~~~
 
 ::
@@ -363,25 +363,25 @@
     req = Request(someurl)
     try:
         response = urlopen(req)
     except URLError, e:
         if hasattr(e, 'reason'):
             print 'We failed to reach a server.'
             print 'Reason: ', e.reason
         elif hasattr(e, 'code'):
             print 'The server couldn\'t fulfill the request.'
             print 'Error code: ', e.code
     else:
         # everything is fine
-        
+
 
 info and geturl
 ===============
 
 The response returned by urlopen (or the :exc:`HTTPError` instance) has two useful
 methods :meth:`info` and :meth:`geturl`.
 
 **geturl** - this returns the real URL of the page fetched. This is useful
 because ``urlopen`` (or the opener object used) may have followed a
 redirect. The URL of the page fetched may not be the same as the URL requested.
 
 **info** - this returns a dictionary-like object that describes the page
@@ -434,66 +434,66 @@
 
 To illustrate creating and installing a handler we will use the
 ``HTTPBasicAuthHandler``. For a more detailed discussion of this subject --
 including an explanation of how Basic Authentication works - see the `Basic
 Authentication Tutorial
 <http://www.voidspace.org.uk/python/articles/authentication.shtml>`_.
 
 When authentication is required, the server sends a header (as well as the 401
 error code) requesting authentication.  This specifies the authentication scheme
 and a 'realm'. The header looks like : ``Www-authenticate: SCHEME
 realm="REALM"``.
 
-e.g. :: 
+e.g. ::
 
     Www-authenticate: Basic realm="cPanel Users"
 
 
 The client should then retry the request with the appropriate name and password
 for the realm included as a header in the request. This is 'basic
 authentication'. In order to simplify this process we can create an instance of
 ``HTTPBasicAuthHandler`` and an opener to use this handler.
 
 The ``HTTPBasicAuthHandler`` uses an object called a password manager to handle
 the mapping of URLs and realms to passwords and usernames. If you know what the
 realm is (from the authentication header sent by the server), then you can use a
 ``HTTPPasswordMgr``. Frequently one doesn't care what the realm is. In that
 case, it is convenient to use ``HTTPPasswordMgrWithDefaultRealm``. This allows
 you to specify a default username and password for a URL. This will be supplied
 in the absence of you providing an alternative combination for a specific
 realm. We indicate this by providing ``None`` as the realm argument to the
 ``add_password`` method.
 
 The top-level URL is the first URL that requires authentication. URLs "deeper"
 than the URL you pass to .add_password() will also match. ::
 
     # create a password manager
-    password_mgr = urllib2.HTTPPasswordMgrWithDefaultRealm()                        
+    password_mgr = urllib2.HTTPPasswordMgrWithDefaultRealm()
 
     # Add the username and password.
-    # If we knew the realm, we could use it instead of ``None``.
+    # If we knew the realm, we could use it instead of None.
     top_level_url = "http://example.com/foo/"
     password_mgr.add_password(None, top_level_url, username, password)
 
-    handler = urllib2.HTTPBasicAuthHandler(password_mgr)                            
+    handler = urllib2.HTTPBasicAuthHandler(password_mgr)
 
     # create "opener" (OpenerDirector instance)
-    opener = urllib2.build_opener(handler)                       
+    opener = urllib2.build_opener(handler)
 
     # use the opener to fetch a URL
-    opener.open(a_url)      
+    opener.open(a_url)
 
     # Install the opener.
     # Now all calls to urllib2.urlopen use our opener.
-    urllib2.install_opener(opener)                               
+    urllib2.install_opener(opener)
 
 .. note::
 
     In the above example we only supplied our ``HHTPBasicAuthHandler`` to
     ``build_opener``. By default openers have the handlers for normal situations
     -- ``ProxyHandler``, ``UnknownHandler``, ``HTTPHandler``,
     ``HTTPDefaultErrorHandler``, ``HTTPRedirectHandler``, ``FTPHandler``,
     ``FileHandler``, ``HTTPErrorProcessor``.
 
 ``top_level_url`` is in fact *either* a full URL (including the 'http:' scheme
 component and the hostname and optionally the port number)
 e.g. "http://example.com/" *or* an "authority" (i.e. the hostname,
@@ -531,49 +531,49 @@
 
 As of Python 2.3 you can specify how long a socket should wait for a response
 before timing out. This can be useful in applications which have to fetch web
 pages. By default the socket module has *no timeout* and can hang. Currently,
 the socket timeout is not exposed at the httplib or urllib2 levels.  However,
 you can set the default timeout globally for all sockets using ::
 
     import socket
     import urllib2
 
     # timeout in seconds
     timeout = 10
-    socket.setdefaulttimeout(timeout) 
+    socket.setdefaulttimeout(timeout)
 
     # this call to urllib2.urlopen now uses the default timeout
     # we have set in the socket module
     req = urllib2.Request('http://www.voidspace.org.uk')
     response = urllib2.urlopen(req)
 
 
 -------
 
 
 Footnotes
 =========
 
 This document was reviewed and revised by John Lee.
 
 .. [#] For an introduction to the CGI protocol see
-       `Writing Web Applications in Python <http://www.pyzine.com/Issue008/Section_Articles/article_CGIOne.html>`_. 
+       `Writing Web Applications in Python <http://www.pyzine.com/Issue008/Section_Articles/article_CGIOne.html>`_.
 .. [#] Like Google for example. The *proper* way to use google from a program
        is to use `PyGoogle <http://pygoogle.sourceforge.net>`_ of course. See
        `Voidspace Google <http://www.voidspace.org.uk/python/recipebook.shtml#google>`_
        for some examples of using the Google API.
 .. [#] Browser sniffing is a very bad practise for website design - building
        sites using web standards is much more sensible. Unfortunately a lot of
        sites still send different versions to different browsers.
 .. [#] The user agent for MSIE 6 is
        *'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; .NET CLR 1.1.4322)'*
 .. [#] For details of more HTTP request headers, see
        `Quick Reference to HTTP Headers`_.
 .. [#] In my case I have to use a proxy to access the internet at work. If you
        attempt to fetch *localhost* URLs through this proxy it blocks them. IE
        is set to use the proxy, which urllib2 picks up on. In order to test
        scripts with a localhost server, I have to prevent urllib2 from using
        the proxy.
-.. [#] urllib2 opener for SSL proxy (CONNECT method): `ASPN Cookbook Recipe 
+.. [#] urllib2 opener for SSL proxy (CONNECT method): `ASPN Cookbook Recipe
        <http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/456195>`_.
- 
+
